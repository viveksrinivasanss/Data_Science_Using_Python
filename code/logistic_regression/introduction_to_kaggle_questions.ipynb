{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Importing Libraries #####\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "##### Setting Options #######\n",
    "matplotlib.style.use('ggplot')\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Read The Train Dataset ######\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Lets See The Description Of The Data #####\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Starts Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Imputing Missing Values for Age ######\n",
    "data['Age'].fillna(\"fill with median value\", inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Gender Vs Survival Chart ######\n",
    "survived_sex = data[data['Survived']==1]['Sex'].value_counts()\n",
    "dead_sex = data[data['Survived']==0]['Sex'].value_counts()\n",
    "df = pd.DataFrame([survived_sex,dead_sex])\n",
    "df.index = ['Survived','Dead']\n",
    "df.plot(kind='bar',stacked=True, figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Embarkment Vs Survival \n",
    "survived_embark = \"fill the same for survived and embarked\"\n",
    "dead_embark = \"repeat the same for non survived and embarked\"\n",
    "df = pd.DataFrame([survived_embark,dead_embark])\n",
    "df.index = ['Survived','Dead']\n",
    "df.plot(kind='bar',stacked=True, figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Age Vs Survival Chart ####\n",
    "figure = plt.figure(figsize=(15,8))\n",
    "plt.hist([data[data['Survived']==1]['Age'],data[data['Survived']==0]['Age']], stacked=True, color = ['g','r'],\n",
    "         bins = 30,label = ['Survived','Dead'])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of passengers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Fare Vs Survival #####\n",
    "figure = plt.figure(figsize=(15,8))\n",
    "plt.hist(\"fill this place to compare fare vs survival\")\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Number of passengers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Age Vs Fare Vs Survival ####\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = plt.subplot()\n",
    "ax.scatter(data[data['Survived']==1]['Age'],data[data['Survived']==1]['Fare'],c='green',s=40)\n",
    "ax.scatter(\"Fill the space for non survived candidates, Give a different color for non survivals\")\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Fare')\n",
    "ax.legend(('survived','dead'),scatterpoints=1,loc='upper right',fontsize=15,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Combining Test And Train Data For Feature Engineering #####\n",
    "def get_combined_data():\n",
    "    # reading train data\n",
    "    train = pd.read_csv('train.csv')\n",
    "    \n",
    "    # reading test data\n",
    "    test = pd.read_csv('test.csv')\n",
    "\n",
    "    # extracting and then removing the targets from the training data \n",
    "    targets = train.Survived\n",
    "    train.drop('Survived',1,inplace=True)\n",
    "    \n",
    "\n",
    "    # merging train data and test data for future feature engineering\n",
    "    combined = train.append(test)\n",
    "    combined.reset_index(inplace=True)\n",
    "    combined.drop('index',inplace=True,axis=1)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = get_combined_data()\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Extracting the passenger titles #####\n",
    "def get_titles():\n",
    "\n",
    "    global combined\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    combined['Title'] = combined.Title.map(Title_Dictionary)\n",
    "    \n",
    "get_titles()\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Filling Missing Values In Age Column #####\n",
    "combined[\"Age\"] = combined.groupby(['Sex','Pclass','Title'])['Age'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_names():\n",
    "    \n",
    "    global combined\n",
    "    # we clean the Name variable\n",
    "    combined.drop('Name',axis=1,inplace=True)\n",
    "    \n",
    "    # encoding in dummy variable\n",
    "    titles_dummies = pd.get_dummies(combined['Title'],prefix='Title')\n",
    "    combined = pd.concat([combined,titles_dummies],axis=1)\n",
    "    \n",
    "    # removing the title variable\n",
    "    combined.drop('Title',axis=1,inplace=True)\n",
    "    \n",
    "process_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Processing Fare ######\n",
    "def process_fares():\n",
    "    \n",
    "    global combined\n",
    "    # there's one missing fare value - replacing it with the mean.\n",
    "    combined.Fare.fillna(combined.Fare.mean(),inplace=True)\n",
    "    \n",
    "process_fares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Processing Embarked #####\n",
    "\n",
    "def process_embarked():\n",
    "    \n",
    "    global combined\n",
    "    # two missing embarked values - filling them with the most frequent one (S)\n",
    "    combined.Embarked.fillna('S',inplace=True)\n",
    "    \n",
    "    # dummy encoding \n",
    "    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')\n",
    "    combined = pd.concat([combined,embarked_dummies],axis=1)\n",
    "    combined.drop('Embarked',axis=1,inplace=True)\n",
    "\n",
    "process_embarked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Processing Cabin #####\n",
    "def process_cabin():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # replacing missing cabins with U (for Unknown)\n",
    "    combined.Cabin.fillna('U',inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    combined['Cabin'] = combined['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(combined['Cabin'],prefix='Cabin')\n",
    "    \n",
    "    combined = pd.concat([combined,cabin_dummies],axis=1)\n",
    "    \n",
    "    combined.drop('Cabin',axis=1,inplace=True)\n",
    "\n",
    "process_cabin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Processing Gender #####\n",
    "def process_gender():\n",
    "    \n",
    "    global combined\n",
    "    # mapping string values to numerical one \n",
    "    combined['Sex'] = combined['Sex'].map({'male':1,'female':0})\n",
    "    \n",
    "process_gender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Processing Pclass\n",
    "\n",
    "def process_pclass():\n",
    "    \n",
    "    global combined\n",
    "    # encoding into 3 categories:\n",
    "    \n",
    "    # adding dummy variables\n",
    "    \n",
    "    \n",
    "    # removing \"Pclass\"\n",
    "    \n",
    "process_pclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ticket():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip() , ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(combined['Ticket'],prefix='Ticket')\n",
    "    combined = pd.concat([combined, tickets_dummies],axis=1)\n",
    "    combined.drop('Ticket',inplace=True,axis=1)\n",
    "\n",
    "ticket  = process_ticket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Processing Family ######\n",
    "\n",
    "def process_family():\n",
    "    \n",
    "    global combined\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    combined['Singleton'] = combined['FamilySize'].map(lambda s : 1 if s == 1 else 0)\n",
    "    combined['SmallFamily'] = combined['FamilySize'].map(\"write a lambda function that return 1 if FamilySize >=2 and <=4 OR 0 otherwise\")\n",
    "    combined['LargeFamily'] = combined['FamilySize'].map(\"write a lambda function that return 1 if FamilySize>5 OR 0 otherwise\")\n",
    "    \n",
    "process_family()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Split Test And Train #####\n",
    "def recover_train_test_target():\n",
    "    global combined\n",
    "    \n",
    "    train0 = pd.read_csv('train.csv')\n",
    "    \n",
    "    targets = train0.Survived\n",
    "    train = combined.ix[0:890]\n",
    "    test = combined.ix[891:]\n",
    "    \n",
    "    return train,test,targets\n",
    "\n",
    "train,test,targets = recover_train_test_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize logistic regression model\n",
    "log_model = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "log_model.fit(X = train,y = targets)\n",
    "\n",
    "# Make predictions\n",
    "preds = log_model.predict(X= train)\n",
    "\n",
    "# Check trained model intercept\n",
    "print (log_model.intercept_)\n",
    "\n",
    "# Check trained model coefficients\n",
    "print (log_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_true=targets,y_pred=preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Dead\",\"Survived\"],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[\"Dead\",\"Survived\"], normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Accuracy Of The Model And Other Metrics\n",
    "print (\"Accuracy: \",log_model.score(X = train ,y = targets))\n",
    "\n",
    "###### View summary of common classification metrics #####\n",
    "print (classification_report(y_true=targets,y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Constructing ROC And AUC ####\n",
    "fpr, tpr, threshold = metrics.roc_curve(targets, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make test set predictions\n",
    "test_preds = log_model.predict(X=test)\n",
    "\n",
    "# Create a submission for Kaggle\n",
    "submission = pd.DataFrame({\"PassengerId\":test[\"PassengerId\"],\n",
    "                           \"Survived\":test_preds})\n",
    "\n",
    "# Save submission to CPassengerId,Survived\n",
    "submission.to_csv(\"tutorial_logreg_submission.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(train, targets)\n",
    "preds = forest.predict(train).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_true=targets,y_pred=preds)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Dead\",\"Survived\"],title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[\"Dead\",\"Survived\"], normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Accuracy Of The Model And Other Metrics\n",
    "print (\"Accuracy: \",forest.score(train,targets))\n",
    "\n",
    "###### View summary of common classification metrics #####\n",
    "print (classification_report(y_true=targets,y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make test set predictions\n",
    "test_preds = forest.predict(X=test)\n",
    "\n",
    "# Create a submission for Kaggle\n",
    "submission = pd.DataFrame({\"PassengerId\":test[\"PassengerId\"],\"Survived\":test_preds})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv(\"tutorial_random_forest_initial_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "clf = ExtraTreesClassifier(n_estimators=200)\n",
    "clf = clf.fit(train, targets)\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = train.columns\n",
    "features['importance'] = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.sort(['importance'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_new = model.transform(train)\n",
    "print (train_new.shape)\n",
    "\n",
    "\n",
    "test_new = model.transform(test)\n",
    "print (test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier(max_features='sqrt')\n",
    "\n",
    "parameter_grid = {\n",
    "                 'max_depth' : [4,5,6,7,8],\n",
    "                 'n_estimators': [200,210,240,250],\n",
    "                 'criterion': ['gini','entropy']\n",
    "                 }\n",
    "\n",
    "cross_validation = StratifiedKFold(targets, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(forest,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(train_new, targets)\n",
    "\n",
    "preds = grid_search.predict(train_new).astype(int)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier(max_features='sqrt')\n",
    "\n",
    "parameter_grid = {\n",
    "                 'max_depth' : [4,5,6,7,8],\n",
    "                 'n_estimators': [200,210,240,250],\n",
    "                 'criterion': ['gini','entropy']\n",
    "                 }\n",
    "\n",
    "cross_validation = StratifiedKFold(targets, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(forest,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(train_new, targets)\n",
    "\n",
    "preds = grid_search.predict(train_new).astype(int)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Accuracy Of The Model And Other Metrics\n",
    "print (\"Accuracy: \",grid_search.score(train_new,targets))\n",
    "\n",
    "###### View summary of common classification metrics #####\n",
    "print (classification_report(y_true=targets,y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Constructing ROC And AUC ####\n",
    "fpr, tpr, threshold = metrics.roc_curve(targets, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make test set predictions\n",
    "test_preds = grid_search.predict(X=test_new)\n",
    "\n",
    "# Create a submission for Kaggle\n",
    "submission = pd.DataFrame({\"PassengerId\":test[\"PassengerId\"],\"Survived\":test_preds})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv(\"tutorial_random_forest_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
